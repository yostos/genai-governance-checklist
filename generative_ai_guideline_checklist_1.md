# 生成AI利用ガイドライン チェックリスト

## 本チェックリストについて

企業が生成AI利用ガイドラインを策定する際の抜け漏れを防ぐためのチェックリストです。

### 参照元の凡例

各項目の末尾に参照元を以下の略称で示しています。

| 略称       | 正式名称                                               |
| ---------- | ------------------------------------------------------ |
| [NIST]     | NIST AI Risk Management Framework (AI RMF 1.0)         |
| [NIST-GAI] | NIST AI RMF Generative AI Profile (NIST-AI-600-1)      |
| [METI]     | 経済産業省・総務省「AI事業者ガイドライン」             |
| [JDLA]     | 日本ディープラーニング協会「生成AIの利用ガイドライン」 |
| [IPA]      | IPA「テキスト生成AI導入・運用ガイドライン」            |
| [FUJITSU]  | 富士通「生成AI利活用ガイドライン」                     |
| [EU-AIA]   | EU AI Act (EU AI規制法)                                |

### チェック凡例

- ☐ 未確認
- ✓ 対応済み
- △ 一部対応（要改善）
- × 未対応（要検討）
- N/A 該当なし

---

## 1. ガバナンス体制 (GOVERN)

### 1.1 組織体制・責任

- [ ] 生成AI利用に関する最終責任者（経営層）が明確に定められている [NIST: GOVERN 2.3] [METI]
  - **説明**: AIで問題が起きたとき誰が最終判断を下すのか、即答できる状態をつくる。責任者が不明確だと、インシデント発生時に判断が宙に浮き、対応の遅れが法的・社会的リスクを組織全体に波及させる。また、名目上の責任者を置くだけでも不十分——実務を担当者に任せきりにし、責任者自身がリスクや運用実態を把握していない体制は「責任者不在」と同義である。
  - **定義例**: 「団体理事長をAI利用の最終責任者とし、利用方針の承認・リスク受容の判断・重大インシデント時の意思決定を自ら行う。定期的に運用状況の報告を受け、方針の妥当性を確認する」
- [ ] AI利用に関する担当部門・担当者が指定されている [NIST: GOVERN 2.1] [JDLA]
  - **説明**: 最終責任者の下で、日常のAI利用を実務面で管理する部門・担当者を明確にする。指定がないと、利用上の疑問やトラブルの相談先が不在となり、現場が自己判断で使い続ける"野良AI"状態を招く。
  - **定義例**: 「総務部をAI利用の管理部門とし、担当者○○がツール選定・利用ルールの周知・問い合わせ対応・インシデント一次対応を担う」
- [ ] 部門横断的な推進体制（法務、情報システム、事業部門等）が構築されている [NIST: GOVERN 3.1] [IPA]
  - **説明**: AI利用は情報セキュリティ・法務・業務の各領域にまたがるため、単一部門だけでは死角が生まれる。「IT部門が技術面だけ見て法的リスクを見落とす」「事業部門が便利さだけ追求しセキュリティを無視する」といった事態が典型的な失敗パターンである。小規模組織であっても、外部の専門家（顧問弁護士等）を含めた連携体制を意識的に構築すべきである。
  - **定義例**: 「AI利用に関する方針検討・リスク判断は、総務部（管理）・業務担当部門・外部顧問（法務・IT）が合同で行い、各視点からの確認を経て決定する」
- [ ] 役割と責任の範囲が文書化されている [NIST: GOVERN 2.1] [METI]
  - **説明**: 「誰が何をやるか」を文書化しておかないと、問題が起きたときに「それは私の担当ではない」と責任の押しつけ合いが始まる。口頭の合意だけでは人事異動や担当変更で引き継がれず、属人的な運用に逆戻りする。
  - **定義例**: 「AI利用に関する役割分担表を作成し、最終責任者・管理担当者・各部門の利用者それぞれの権限と義務を明記する。担当者変更時は必ず更新し、全関係者に共有する」
- [ ] インシデント発生時のエスカレーションルートが定められている [NIST: GOVERN 1.5] [IPA]
  - **説明**: AIで情報漏えいや誤出力が発生したとき、「誰に・どの順番で・何を報告するか」が決まっていなければ、初動が遅れ被害が拡大する。現場で問題に気づいた人が報告先を探しているうちに、対応のゴールデンタイムを逃すのは典型的な失敗である。
  - **定義例**: 「AIに関する問題を発見した場合、①まずAI管理担当者に口頭またはチャットで即時報告、②管理担当者が30分以内に最終責任者へ報告、③最終責任者が対応方針を判断し指示する。連絡先リストを全従業員に配布する」
- [ ] 多様な視点を持つチーム構成が考慮されている [NIST: GOVERN 3.1]
  - **説明**: AIのリスクは技術的な問題だけでなく、倫理・文化・利用者の多様性に関わる問題も含む。同質的なメンバーだけで意思決定すると、特定の立場からは明白なリスクが見過ごされやすい。性別・年齢・職種・専門分野など、多様な視点を意識的にチーム構成に取り入れることが重要である。
  - **定義例**: 「AI利用ルールの策定・見直しにあたっては、管理部門だけでなく、異なる業務経験・年齢層・職種の職員を含むレビュー体制とし、偏った視点にならないよう配慮する」

### 1.2 ポリシー・規程

- [ ] 生成AI利用の目的・方針が明記されている [NIST: GOVERN 1.2] [JDLA]
  - **説明**: 「なぜ生成AIを使うのか」「どういう方針で使うのか」が明文化されていないガイドラインは、単なるルール集にすぎず現場の納得感を得られない。目的なき規制は形骸化し、目的なき自由は暴走を招く。
  - **定義例**: 「本ガイドラインは、業務効率の向上と創造性の支援を目的として生成AIを活用する際の基本方針を定める。AIはあくまで人間の判断を補助するツールとして位置づけ、最終的な意思決定は人間が行う」
- [ ] ガイドラインの適用範囲（対象者、対象業務、対象ツール）が明確である [JDLA] [IPA]
  - **説明**: 「誰が」「どの業務で」「どのツールについて」このガイドラインに従うのかが不明確だと、「自分は対象外」と解釈する余地が生まれ、ルールが空文化する。正規職員だけが対象なのか、委託先や実習生も含むのか——範囲の曖昧さはリスクの温床になる。
  - **定義例**: 「本ガイドラインは、当団体のすべての役職員（常勤・非常勤・委託スタッフを含む）が、業務上利用するすべての生成AIサービス（ChatGPT、Copilot等、団体が承認したツール）に適用する」
- [ ] 既存の情報セキュリティポリシー、個人情報保護方針との整合性が取れている [NIST: GOVERN 1.4] [METI]
  - **説明**: 生成AIガイドラインが既存の情報セキュリティポリシーや個人情報保護方針と矛盾していると、現場は「どちらに従えばいいのか」と混乱する。既存ポリシーで「外部クラウドへの機密情報送信禁止」と定めているのに、AIガイドラインでその点に触れていなければ、現場が独自解釈で運用してしまう。
  - **定義例**: 「本ガイドラインは既存の情報セキュリティポリシー及び個人情報保護方針との整合性を確認済みである。AI固有の事項については本ガイドラインが優先し、記載のない事項は既存ポリシーに従う」
- [ ] 関連する法令・規制要件（個人情報保護法、著作権法等）への言及がある [NIST: GOVERN 1.1] [JDLA]
  - **説明**: 生成AI利用には個人情報保護法、著作権法、不正競争防止法など複数の法令が関わる。ガイドラインで法令に触れていないと、「法的リスクは考慮済み」と現場が誤解し、知らないうちに法令違反を犯すおそれがある。すべてを網羅する必要はないが、最低限の関連法令を明示し、判断に迷う場合の相談先を示すことが重要である。
  - **定義例**: 「生成AIの利用にあたっては、個人情報保護法・著作権法・不正競争防止法を遵守する。法的判断に迷う場合は、AI管理担当者を通じて顧問弁護士に相談する」
- [ ] ガイドラインの定期的な見直し・更新の仕組みが定められている [NIST: GOVERN 1.5] [METI]
  - **説明**: 生成AI技術と関連法規制は急速に変化するため、策定時点のガイドラインは半年もすれば陳腐化しうる。「一度つくって終わり」のガイドラインは変化に対応できず形骸化する。更新のタイミング・手順・責任者を予め決めておくことが、ガイドラインを"生きた文書"にする条件である。
  - **定義例**: 「本ガイドラインは少なくとも年1回（毎年○月）に見直しを行う。AIに関する重大な法改正やインシデント発生時には臨時の見直しを実施する。見直しはAI管理担当者が起案し、最終責任者が承認する」
- [ ] リスク許容度が組織として定義されている [NIST: MAP 1.5] [METI]
  - **説明**: 「どの程度のリスクなら許容するか」を組織として決めていないと、担当者ごとに判断基準がバラつき、同じ案件でも人によって可否が分かれる。リスク許容度が明確であれば現場判断に一貫性が生まれ、個人の裁量に過度に依存する状態を防げる。
  - **定義例**: 「公開情報の要約・翻訳支援など、情報漏えいリスクが低く出力誤りの影響も限定的な業務はAI利用を許可する。個人情報・機密情報を扱う業務、対外的な意思決定に関わる業務ではAI利用を制限または禁止する。判断が困難な場合はAI管理担当者の承認を得る」

### 1.3 教育・研修

- [ ] 従業員向けの生成AI利用研修が計画されている [NIST: GOVERN 2.2] [JDLA]
  - **説明**: ガイドラインは策定しただけでは機能しない——読まれない規程は存在しないのと同じである。研修を通じて「なぜこのルールがあるのか」を理解させなければ、現場はルールを面倒な制約としか捉えず、形だけの遵守か無視が常態化する。
  - **定義例**: 「生成AI利用開始前に全職員を対象とした初回研修（90分）を実施する。内容はガイドラインの要点、禁止事項、入力してはいけない情報の具体例、インシデント報告手順とする。受講記録を管理し、未受講者にはAI利用権限を付与しない」
- [ ] ITリテラシーレベルに応じた教育内容が用意されている [JDLA] [IPA]
  - **説明**: 全員に同じ研修を実施しても、ITに詳しい人には退屈で、不慣れな人には理解が追いつかない。結果として両者とも研修の効果を実感できず、「やった感」だけが残る。対象者のレベルに合わせた教育設計が、実効性ある研修の前提条件である。
  - **定義例**: 「研修は2段階で実施する。基礎編（全職員対象）ではAIの概要・リスク・禁止事項を扱い、実践編（日常的にAIを使う職員対象）ではプロンプト設計・出力検証の方法・業務活用事例を扱う」
- [ ] 新規利用者向けのオンボーディングプロセスがある [NIST: GOVERN 2.2]
  - **説明**: 新しく入った職員が、既存メンバーと同じ前提知識を持っているとは限らない。入職後に「AI使っていいですよ」とだけ言われ、ガイドラインの存在すら知らないまま使い始めるケースは十分ありうる。新規利用者が必ず通る導入プロセスを設けることで、知識のばらつきを防ぐ。
  - **定義例**: 「新規入職者には、入職時オリエンテーションの一環としてAI利用ガイドラインの説明と基礎研修の受講を義務づける。研修完了後にAIツールのアカウントを発行する」
- [ ] 継続的な啓発活動（事例共有、注意喚起等）の仕組みがある [NIST: GOVERN 4.3] [JDLA]
  - **説明**: 一度研修を受けても、日常業務に忙殺されるうちにルールの記憶は薄れる。他社のAI関連トラブル事例や、自組織での良い活用例を定期的に共有することで、ガイドラインの存在意義を繰り返し意識させる。啓発が途絶えると、ガイドラインは「入社時に読んだきりの文書」に退化する。
  - **定義例**: 「四半期に1回、AI利用に関するミニ勉強会（30分）を実施し、最新のリスク事例・活用好事例・ガイドライン改訂内容を共有する。また、重大な外部事例が発生した場合は、速やかに全職員にメール等で注意喚起する」

### 1.4 監視・監査

- [ ] 利用状況のモニタリング方法が定められている [NIST: GOVERN 1.5] [IPA]
  - **説明**: 「ガイドラインを守っているか」を確認する手段がなければ、ルールは絵に描いた餅になる。誰がどのツールをどの頻度で使い、どんな用途に使っているかを把握できなければ、問題の早期発見も改善もできない。
  - **定義例**: 「AI管理担当者は月1回、各AIツールの利用ログ（利用者・利用頻度・主な用途）を確認し、ガイドライン違反の有無や想定外の利用がないかを点検する」
- [ ] 定期的な監査・レビューの実施計画がある [NIST: GOVERN 1.5] [METI]
  - **説明**: モニタリングが日常の健康管理だとすれば、監査は定期健診にあたる。日々の点検では見落とす構造的な問題——ルール自体の陳腐化や、組織全体の運用傾向——を発見するには、定期的に一歩引いた視点でレビューする場が必要である。
  - **定義例**: 「年1回（○月）にAI利用に関する内部監査を実施する。ガイドライン遵守状況、リスク対応状況、利用効果を評価し、結果を最終責任者に報告する」
- [ ] 違反時の対応プロセスが明記されている [JDLA] [IPA]
  - **説明**: 違反が発覚したとき、対応が場当たり的だと「あの人は注意されたのに、この人はお咎めなし」と不公平感が生まれ、ルールの信頼性が崩壊する。軽微な違反と重大な違反を区別し、段階的な対応を予め定めておくことが公正な運用の基盤になる。
  - **定義例**: 「違反を確認した場合、①軽微な違反（過失による禁止情報の入力等）は口頭注意と再研修、②重大な違反（意図的な機密情報の入力、悪用目的の利用等）はAI利用権限の停止と懲戒規定に基づく対応を行う。すべての違反対応を記録する」
- [ ] AIシステムのインベントリ管理の仕組みがある [NIST: GOVERN 1.6]
  - **説明**: 組織内で「誰が・何のAIツールを・どの業務で使っているか」を一覧で把握できていないと、未承認ツールの利用や、サービス終了への対応が後手に回る。管理対象を可視化することがガバナンスの出発点である。
  - **定義例**: 「利用中のAIツール・サービスの一覧表を作成し、ツール名・提供元・利用部門・利用目的・契約状況・データ取扱条件を記載する。新規導入・廃止時に随時更新する」
- [ ] AIシステムの安全な廃止・移行プロセスがある [NIST: GOVERN 1.7]
  - **説明**: AIツールの利用を終了する際に、保存されたプロンプト履歴や生成データをそのまま放置すれば、情報漏えいのリスクが残り続ける。「使い始め」だけでなく「使い終わり」にもルールが必要である。
  - **定義例**: 「AIツールの利用を終了する場合、①ツール上に保存された業務データ・履歴を削除またはエクスポート、②アカウントの無効化、③代替手段への移行計画の策定を行い、AI管理担当者が完了を確認する」

---

## 2. リスクの特定と評価 (MAP)

### 2.1 利用目的・文脈の明確化

- [ ] 生成AIを利用する業務・ユースケースが特定されている [NIST: MAP 1.1] [JDLA]
  - **説明**: 「何にでも使っていい」は「何に使うか考えていない」と同義である。利用する業務を特定しないまま導入すると、効果測定もリスク評価もできず、問題が起きたとき「想定外の使い方だった」と言い訳するしかなくなる。
  - **定義例**: 「生成AIの利用を認める業務として、①文書の草稿作成、②調査・情報収集の補助、③翻訳・要約の支援、④アイデア出し・ブレインストーミングを指定する。それ以外の業務での利用はAI管理担当者の事前承認を要する」
- [ ] 各ユースケースにおける期待される効果（便益）が文書化されている [NIST: MAP 3.1] [METI]
  - **説明**: 「便利だから」だけでは導入の根拠にならない。期待する効果を具体的に書き出しておくことで、導入後に「実際に役立っているか」を評価でき、効果が薄い用途の見直しや、有効な用途の拡大につなげられる。
  - **定義例**: 「各ユースケースについて、期待する効果（例：議事録作成の所要時間を半減、定型文書の初稿作成を自動化）を記録し、半年後に実績と比較して効果を検証する」
- [ ] 想定ユーザーとその利用方法が定義されている [NIST: MAP 1.1] [METI]
  - **説明**: 同じAIツールでも、使う人の知識レベルや業務内容によってリスクの大きさが変わる。経理担当者が財務データを入力するリスクと、広報担当者がプレスリリースの草案を作るリスクは本質的に異なる。想定ユーザーを明確にすることで、リスクに応じた対策が可能になる。
  - **定義例**: 「主な利用者は事務職員（文書作成補助）および広報担当（コンテンツ草案作成）とする。各利用者は、自身の業務範囲内でのみAIを利用し、他部門のデータを扱う場合は当該部門の承認を得る」
- [ ] AIを使用すべきでない業務・場面が明記されている [NIST: MAP 1.1] [JDLA]
  - **説明**: 「使ってよい場面」を列挙するだけでは、グレーゾーンの判断に迷う。「使ってはいけない場面」を明示することで、禁止ラインが明確になり、現場が安心して判断できるようになる。ネガティブリストの欠如は、重大なリスクを見落とす原因になる。
  - **定義例**: 「以下の業務・場面ではAI利用を禁止する：①個人の評価・処遇に関する判断、②法的効力を持つ文書の最終作成、③本人確認・認証に関わる業務、④外部への正式な回答・声明の作成」
- [ ] ビジネス価値・目的が明確に定義されている [NIST: MAP 1.4]
  - **説明**: AI導入が「流行に乗る」ための施策になっていないかを確認する視点である。組織にとっての具体的な価値——コスト削減、品質向上、業務時間の創出——が言語化されていなければ、投資対効果の判断も撤退判断もできない。
  - **定義例**: 「生成AI導入のビジネス価値を、①定型業務の作業時間削減による本来業務への集中、②文書品質の底上げによる手戻り削減と定義し、導入後に定量的な評価を行う」
- [ ] 組織のミッション・目標との整合性が確認されている [NIST: MAP 1.3]
  - **説明**: AI利用が組織の理念や目標と矛盾していないかを確認する。たとえば「人に寄り添うサービス」を掲げる組織が、顧客対応を全面的にAIに置き換えるのは本末転倒である。ツールの導入が組織の方向性を歪めないよう、上位の目標と照合する習慣が必要である。
  - **定義例**: 「生成AIの利用方針が当団体の事業計画・行動指針と整合していることを、年次の方針見直し時に確認する」

### 2.2 リスクの分類と評価

- [ ] 情報漏えいリスクの評価がされている [JDLA] [IPA] [FUJITSU]
  - **説明**: 生成AIに入力した情報は、サービス提供元のサーバーに送信される。「学習に利用しない」設定だけでは不十分で、秘密保持義務のないサービスに機密情報を入力すれば、実質的に情報が外部に渡ったのと同じである。どの情報がどの経路で漏えいしうるかを具体的に洗い出すことが出発点となる。
  - **定義例**: 「生成AI利用における情報漏えいリスクを、情報種別（機密・社内限・公開）×漏えい経路（入力データの学習利用・履歴閲覧・サーバー保存）のマトリクスで評価し、対策を定める」
- [ ] 著作権・知的財産権侵害リスクの評価がされている [JDLA] [FUJITSU] [METI]
  - **説明**: 生成AIの出力は既存の著作物と類似する可能性がある。利用者が元の著作物を知らなくても、AIが学習データに基づいて類似物を生成した場合、著作権侵害の「依拠性あり」と推認されうる（文化庁見解）。「AIが出したから大丈夫」は通用しない。
  - **定義例**: 「生成AI出力物の著作権侵害リスクを評価する。特に画像・デザイン・文章の生成では、既存著作物との類似性を確認するプロセスを設け、疑わしい場合は利用を控える」
- [ ] ハルシネーション（誤情報生成）リスクの評価がされている [NIST-GAI] [JDLA] [FUJITSU]
  - **説明**: 生成AIは「もっともらしい嘘」を自信満々に出力する。正誤判断を行わず確率予測で文章を生成する仕組み上、これは不具合ではなく構造的な特性である。どの業務で誤情報のリスクが高いか、誤情報が流通した場合の影響はどの程度かを予め評価しておくことが重要である。
  - **定義例**: 「ハルシネーションリスクを業務別に評価する。事実確認が容易な業務（要約・翻訳）は低リスク、専門知識が必要な業務（法務・技術調査）は高リスクと分類し、高リスク業務では必ず専門家によるファクトチェックを行う」
- [ ] バイアス・公平性に関するリスクの評価がされている [NIST: MAP 5.1] [METI] [FUJITSU]
  - **説明**: 学習データの偏りにより、AIは特定の属性（性別・人種・年齢等）に対して不公平な出力を行うことがある。「CEOの画像」を生成すると白人男性ばかり、「看護師」を生成すると女性ばかりになるのは代表的な例である。人事・評価・顧客対応にAIを使う場合、バイアスは直接的な差別につながりうる。
  - **定義例**: 「AI出力が特定の属性に不利益をもたらすバイアスを含んでいないか、利用前に確認する。人に関する判断補助にAIを用いる場合は、バイアスリスクを『高』と評価し、複数の視点からのレビューを義務づける」
- [ ] プライバシーリスクの評価がされている [NIST: MAP 5.1] [METI]
  - **説明**: 生成AIは、入力されたデータだけでなく、学習データに含まれる個人情報を出力に反映することもある。意図せず他者のプライバシーを侵害する可能性があり、入力側・出力側の両面でリスクを評価する必要がある。
  - **定義例**: 「プライバシーリスクを入力面（個人情報を含むデータの入力有無）と出力面（個人を特定しうる情報の生成有無）の2軸で評価し、リスクが高い業務では匿名化または利用制限を適用する」
- [ ] セキュリティリスク（プロンプトインジェクション等）の評価がされている [NIST-GAI] [IPA]
  - **説明**: 生成AIには、悪意あるプロンプトでガードレールを回避させる「プロンプトインジェクション」や、学習データを汚染する「データポイズニング」など、従来のITとは異なるセキュリティ脅威がある。これらの攻撃手法を理解し、自組織の利用形態で影響を受けうるかを評価しておく必要がある。
  - **定義例**: 「生成AIに特有のセキュリティリスク（プロンプトインジェクション、情報抽出攻撃等）を洗い出し、自組織の利用形態（社内利用のみ／顧客向けチャットボット等）に応じた影響度を評価する」
- [ ] 法的リスク（契約違反、規制違反等）の評価がされている [NIST: GOVERN 1.1] [METI]
  - **説明**: AI利用が契約上の守秘義務違反、著作権法違反、個人情報保護法違反、あるいは業界固有の規制違反に該当する可能性がないかを事前に評価する。「法令違反の認識がなかった」は免責事由にならない。
  - **定義例**: 「AI利用に関連する法的リスクを、個人情報保護法・著作権法・不正競争防止法・各業界規制の観点で評価し、リスクが高い領域を特定して対策を講じる」
- [ ] レピュテーションリスクの評価がされている [NIST: MAP 5.1] [FUJITSU]
  - **説明**: AIが不正確な情報や不適切なコンテンツを生成し、それが組織名で外部に出た場合、信用の失墜は法的リスク以上に深刻な打撃を与えうる。検証せずにAIの回答をそのまま社会に出すことは、顧客や社会からの「信頼」を損なう行為である。
  - **定義例**: 「AI出力を外部に公開する業務（広報・顧客対応・公式文書等）について、誤情報・不適切表現によるレピュテーションリスクを評価し、公開前に必ず人間によるレビューを実施する」
- [ ] 業務依存リスク（過度な依存、スキル低下）の評価がされている [NIST-GAI] [FUJITSU]
  - **説明**: AIに頼りすぎると、人間自身の判断力や業務スキルが徐々に低下する。AIサービスが停止した場合に業務が止まる「依存リスク」と、長期的にスキルが衰退する「能力喪失リスク」の両面から評価が必要である。
  - **定義例**: 「AI利用が長期化した場合の業務依存度を評価する。AIなしでも最低限の業務遂行が可能な体制を維持し、定期的にAIを使わない業務訓練や手動プロセスの確認を行う」
- [ ] 環境負荷・サステナビリティの考慮がされている [NIST: MEASURE 2.12] [METI]
  - **説明**: 大規模言語モデルの学習・推論には膨大なエネルギーと水資源が消費される。組織の環境方針やSDGs目標との整合性を確認し、必要以上に大きなモデルを不必要に利用していないかを検討する視点も重要である。
  - **定義例**: 「AI利用の環境負荷を認識し、業務に必要十分なモデル・サービスを選択する。組織の環境方針がある場合は、AI利用方針との整合性を確認する」

### 2.3 サードパーティリスク

- [ ] 利用するAIサービス・ツールの選定基準が定められている [NIST: GOVERN 6.1] [IPA]
  - **説明**: 「無料だから」「話題だから」で選んだツールが、実は入力データを学習に利用していたり、セキュリティ対策が不十分だったりするケースは多い。選定基準がなければ、部門ごとにバラバラなツールが乱立し、管理コストとリスクが膨らむ。
  - **定義例**: 「AIツールの選定にあたっては、①データの取扱方針（学習利用の有無）、②セキュリティ対策（通信暗号化・データ保存場所）、③利用規約の内容、④提供元の信頼性を確認し、AI管理担当者が承認した上で利用を開始する」
- [ ] ベンダーの利用規約・プライバシーポリシーの確認プロセスがある [NIST: MAP 4.1] [JDLA]
  - **説明**: 利用規約を読まずに「同意する」をクリックするのは個人の自由だが、組織の業務データを扱うサービスでは許されない。規約の中に「入力データを学習に利用する」「生成物の権利はサービス提供者に帰属する」といった条項が含まれている可能性がある。
  - **定義例**: 「AIツール導入前に、AI管理担当者が利用規約・プライバシーポリシーを確認し、データの学習利用有無、生成物の権利帰属、サービス提供者のデータアクセス範囲を把握する。重大な懸念がある場合は顧問弁護士に相談する」
- [ ] データの取り扱い（学習利用の有無等）の確認がされている [JDLA] [IPA]
  - **説明**: 入力データが学習に使われる場合、自社の情報が他のユーザーへの応答に反映される可能性がある。「学習に利用しない」オプションがあっても、デフォルト設定で有効になっているとは限らない。設定画面まで確認してはじめて「確認済み」と言える。
  - **定義例**: 「利用する各AIサービスについて、入力データの学習利用有無、データ保存期間、サーバー所在地を確認・記録する。学習利用のオプトアウト設定がある場合は確実にオフに設定し、その設定状況を記録する」
- [ ] サービス停止・変更時の対応策が検討されている [NIST: GOVERN 6.2]
  - **説明**: 特定のAIサービスに業務を依存した状態でサービスが突然停止・仕様変更されると、業務が止まる。クラウドサービスには永続の保証がない。代替手段や移行計画を事前に考えておくことが、事業継続の観点から不可欠である。
  - **定義例**: 「主要AIツールの停止・大幅変更に備え、①代替ツールの候補を予め把握し、②重要業務についてはAIなしでも遂行できる手順を維持する。サービス停止時はAI管理担当者が48時間以内に代替策を提示する」
- [ ] サプライチェーン全体でのリスク管理が考慮されている [NIST: MAP 4.2] [EU-AIA]
  - **説明**: 自組織が利用するAIサービスの裏側には、基盤モデル提供者・クラウドインフラ事業者・データ提供者が連なっている。自組織の直接の契約先だけでなく、その先の構造的リスク——基盤モデルの脆弱性、学習データの品質問題——も影響しうることを認識する必要がある。
  - **定義例**: 「利用するAIサービスについて、基盤モデルの提供元・データ処理の所在地・再委託先の有無を可能な範囲で把握し、サプライチェーン上のリスクをAI管理台帳に記録する」

---

## 3. 入力データの管理

### 3.1 入力禁止情報

- [ ] 入力してはいけない情報の種類が明確にリスト化されている [JDLA] [FUJITSU]
  - **説明**: 「気をつけて使ってください」では何に気をつけるべきか分からない。入力禁止情報を具体的にリスト化することで、現場が迷わず判断できる。抽象的な注意喚起だけのガイドラインは、最も重要な場面で役に立たない。
  - **定義例**: 「以下の情報は生成AIへの入力を禁止する。判断に迷う場合はAI管理担当者に確認すること」（以下のサブカテゴリを列挙）
  - [ ] 個人情報（氏名、住所、電話番号、メールアドレス等）[JDLA] [FUJITSU]
    - **説明**: 個人を特定できる情報の入力は、個人情報保護法違反のリスクに直結する。「名前だけなら大丈夫」と思いがちだが、複数の情報を組み合わせれば個人が特定される。
    - **定義例**: 「氏名・住所・電話番号・メールアドレス・生年月日・マイナンバー等、個人を特定しうる情報は入力しない。匿名化（仮名への置き換え等）した上であれば利用可とする」
  - [ ] 機密情報（営業秘密、未公開情報、戦略情報等）[JDLA] [FUJITSU]
    - **説明**: 営業秘密や未公開の戦略情報がAIサービス経由で漏えいした場合、不正競争防止法上の保護を失う可能性がある。一度失われた秘密性は取り戻せない。
    - **定義例**: 「営業秘密、未公開の事業戦略・製品情報、非公開の研究データ等は入力しない」
  - [ ] 顧客情報（取引先情報、契約内容等）[JDLA] [FUJITSU]
    - **説明**: 顧客から預かった情報は、自組織の情報以上に慎重に扱う必要がある。顧客との契約で守秘義務が課されている場合、AI入力が契約違反に該当しうる。
    - **定義例**: 「取引先名・契約条件・見積金額・顧客固有の業務情報等は入力しない。顧客情報を参照する必要がある場合は、固有名を伏せた上で利用する」
  - [ ] 認証情報（パスワード、APIキー、アクセストークン等）[IPA]
    - **説明**: 認証情報がAIの履歴やログに残れば、それ自体が重大なセキュリティ脆弱性となる。「エラーが出たのでログをAIに貼って解析」する際にAPIキーを含めてしまう事故は実際に起きている。
    - **定義例**: 「パスワード・APIキー・アクセストークン・秘密鍵等の認証情報は絶対に入力しない。ソースコードを入力する際も、認証情報が含まれていないことを事前に確認する」
  - [ ] 財務情報（未公開の決算情報、インサイダー情報等）[JDLA]
    - **説明**: 未公開の財務情報の外部漏えいは、インサイダー取引規制や金融商品取引法上の問題に発展しうる。特に上場企業やその関連組織では致命的なリスクとなる。
    - **定義例**: 「未公開の決算情報・予算情報・投資計画・M&A情報等は入力しない」
  - [ ] 人事情報（評価、給与、健康情報等）[JDLA]
    - **説明**: 人事評価・給与・健康情報は、漏えいした場合に当事者への精神的被害が特に大きい。健康情報は「要配慮個人情報」として法令上も厳格な取扱いが求められる。
    - **定義例**: 「人事評価・給与・賞与・健康診断結果・休職情報等は入力しない」
  - [ ] 法的に保護された情報（弁護士秘匿特権対象情報等）[JDLA]
    - **説明**: 弁護士との相談内容や訴訟戦略など法的秘匿特権の対象となる情報は、AIに入力することで秘匿特権を喪失するリスクがある。法的紛争において重大な不利益を招く。
    - **定義例**: 「弁護士との相談内容、訴訟関連文書、法的秘匿特権の対象となる情報は入力しない」

### 3.2 データ分類と取り扱い

- [ ] 情報の機密度分類に応じた利用可否基準がある [IPA] [JDLA]
  - **説明**: すべての情報を一律に「入力禁止」にすると業務でAIを活用できず、一律に「入力可」にするとリスクが管理できない。情報を機密度でランク分けし、ランクごとに「可・条件付き可・不可」を定めることで、現場が実用的に判断できる基準をつくれる。
  - **定義例**: 「情報を3段階（公開情報・社内限定・機密）に分類する。公開情報はAI入力可、社内限定情報は匿名化処理後に入力可、機密情報はAI入力不可とする」
- [ ] 匿名化・仮名化のガイダンスが提供されている [METI] [JDLA]
  - **説明**: 「匿名化すればAIに入力してよい」と言われても、何をどう匿名化すればいいか分からなければ実行できない。「名前をイニシャルにする」だけでは不十分な場合もある。具体的な手順と判断基準を示すことが、ルールの実効性を左右する。
  - **定義例**: 「AI入力前の匿名化手順として、①個人名→仮名（A氏・B氏等）に置換、②組織名→一般名称（X社等）に置換、③住所・電話番号・メールアドレスを削除、④複数情報の組合せで個人が特定されないことを確認する」
- [ ] 第三者から受領した情報の取り扱いルールがある [JDLA]
  - **説明**: 取引先や顧客から預かった情報には、NDA（秘密保持契約）で守秘義務が課されていることが多い。AIに入力することが「第三者への開示」に該当するかは契約内容によるが、安全側に倒して扱うべきである。「自社の情報なら自分で判断できるが、他者の情報は勝手に使えない」が基本原則である。
  - **定義例**: 「第三者から受領した情報は、原則としてAIに入力しない。やむを得ず入力する場合は、当該情報に適用される守秘義務の範囲を確認し、必要に応じて情報提供元の承諾を得る」
- [ ] 契約上の守秘義務との整合性が確認されている [JDLA]
  - **説明**: 既存の取引先契約やNDAに「外部サービスへの情報入力禁止」が含まれている場合、AI利用がそれに抵触する可能性がある。ガイドライン策定時に既存契約との整合を確認しておかないと、善意のAI活用が契約違反になりかねない。
  - **定義例**: 「主要な取引先との契約・NDAにおける守秘義務条項を確認し、AI入力が守秘義務に抵触しないことを検証する。抵触のおそれがある場合は、当該取引先の情報をAI入力禁止リストに追加する」

### 3.3 個人情報保護

- [ ] 個人データの入力に関する具体的なルールがある [JDLA] [METI]
  - **説明**: 個人情報保護法は個人データの「第三者提供」を制限しているが、AIへの入力が第三者提供に該当するかは利用形態による。原則禁止とした上で、匿名化による例外を認めるなど、段階的なルールを設けることで現場の混乱を防ぐ。
  - **定義例**: 「個人データ（特定の個人を識別できる情報）は原則としてAIに入力しない。業務上やむを得ない場合は、①匿名化処理を行う、②匿名化が困難な場合はAI管理担当者の承認を得る」
- [ ] 本人同意の要否・取得方法が明記されている [METI] [JDLA]
  - **説明**: 個人情報をAIに入力する場合、利用目的の範囲内か、本人の同意が必要かを判断できなければならない。プライバシーポリシーに「AI利用」が含まれていなければ、同意を得ていないのと同じである。
  - **定義例**: 「個人情報をAIに入力する場合、既存のプライバシーポリシーでAI利用が利用目的に含まれているかを確認する。含まれていない場合は、プライバシーポリシーを改定し本人への通知を行うか、個別に本人同意を取得する」
- [ ] 委託先への提供に該当するかの判断基準がある [METI] [JDLA]
  - **説明**: AIサービスへのデータ入力が個人情報保護法上の「委託」に該当する場合、委託先の監督義務が発生する。一方、サービス提供者側がデータにアクセスしない構成であれば委託に該当しない場合もある。利用形態に応じた判断基準が必要である。
  - **定義例**: 「AIサービスへの個人データ入力が個人情報保護法上の『委託』に該当するかを、サービスの利用形態（データへのアクセス権限の有無等）に基づき判断する。委託に該当する場合は、委託先の管理体制を確認し、必要な契約を締結する」
- [ ] 越境移転（海外サーバー利用）への対応が検討されている [METI] [EU-AIA]
  - **説明**: 多くの生成AIサービスは海外（主に米国）のサーバーで処理される。個人データを海外サーバーに送信することは「外国にある第三者への提供」に該当しうる。個人情報保護法は越境移転に追加的な要件を課しており、利用するサービスのデータ処理場所の確認は必須である。
  - **定義例**: 「利用するAIサービスのデータ処理サーバー所在地を確認する。個人データが海外サーバーで処理される場合は、個人情報保護法の越境移転規定に基づき、本人への情報提供または適切な契約の締結等の対応を行う」

---

## 4. 出力の管理と利用

### 4.1 出力の検証義務

- [ ] 出力内容の正確性確認（ファクトチェック）が義務付けられている [NIST: MEASURE 2.5] [JDLA] [FUJITSU]
  - **説明**: 生成AIの出力は「もっともらしいが正しいとは限らない」。自然な文章であるほど誤りに気づきにくく、検証せずに利用した結果、誤情報を社外に出してしまえば組織の信頼を失う。ファクトチェックは手間ではなく、AI活用の最低限の安全装置である。
  - **定義例**: 「AI出力を業務に利用する場合、事実情報（数値・日付・法令名・人名・固有名詞等）は必ず一次情報源で裏取りを行う。裏取りが困難な情報は、AI出力である旨を付記して共有する」
- [ ] 確認すべき項目・観点が具体的に示されている [JDLA] [FUJITSU]
  - **説明**: 「出力を確認してください」だけでは、何をどう確認すればよいか分からない。確認観点を具体的に示すことで、形式的な「確認しました」ではなく、実質的な品質チェックが行われるようになる。
  - **定義例**: 「AI出力の確認では以下の観点を必ず検証する：①事実の正確性（数値・日付・固有名詞）、②論理の整合性、③不適切な表現（差別的・攻撃的表現）の有無、④機密情報の意図しない含有の有無」
- [ ] 専門分野（法務、財務、技術等）における専門家レビューの要否基準がある [NIST: MEASURE 1.3] [JDLA]
  - **説明**: 法律の解釈、財務上の判断、技術的な正確性は、一般的な知識では検証できない。専門領域のAI出力を非専門家だけで「確認済み」とするのは、確認していないのと同じである。どの領域で専門家レビューが必要かを予め決めておくことが重要である。
  - **定義例**: 「AI出力が以下の領域に関わる場合は、各分野の専門家（または外部顧問）によるレビューを必須とする：①法的判断・契約内容、②財務・会計処理、③技術的な安全性、④医療・健康に関する情報」
- [ ] 出力をそのまま使用してはいけない場面が明記されている [JDLA] [FUJITSU]
  - **説明**: 入力の禁止事項と同様に、出力の利用にも「ここでは使うな」という明確な線引きが必要である。特に対外的な公式文書や法的効力を持つ文書で無加工のAI出力を使うのは、組織としてのガバナンスの欠如を示すことになる。
  - **定義例**: 「以下の場面ではAI出力をそのまま使用してはならない：①対外公式文書（契約書・プレスリリース・公式声明）、②法的効力を持つ文書、③人事評価・処遇に関する文書。これらの用途では草案としてのみ利用し、必ず人間が加筆・修正・承認を行う」
- [ ] 人間による最終確認・承認プロセスがある [NIST: MAP 3.5] [METI]
  - **説明**: AIはあくまで「下書きをつくる道具」であり、最終的な品質と正確性に責任を負うのは人間である。出力の重要度に応じて「誰が」「どのタイミングで」確認・承認するかを定めることで、ヒューマン・イン・ザ・ループの原則を実現する。
  - **定義例**: 「AI出力を業務に利用する場合、①社内利用のみの場合は利用者自身が確認、②対外利用の場合は上長の承認を必須とする。重大な意思決定に関わる場合はAI管理担当者にも確認を依頼する」

### 4.2 著作権・知的財産

- [ ] 生成物の著作権の帰属に関する考え方が示されている [JDLA] [METI]
  - **説明**: AIが生成した文章や画像に著作権が発生するかは、人間の「創作的寄与」の程度による。プロンプトを入力しただけでは著作権が発生しない可能性がある一方、人間が大幅に加筆・編集すれば著作物となりうる。この曖昧さを組織として整理しておく必要がある。
  - **定義例**: 「AI生成物の著作権について、以下の方針を定める：①AI出力をそのまま使用した場合は著作権が発生しない前提で取り扱う、②人間が実質的な加筆・編集を行った場合は、編集者の著作物として扱う。利用するAIサービスの規約で生成物の権利帰属がどう定められているかも確認する」
- [ ] 既存著作物との類似性チェックの必要性が言及されている [JDLA] [FUJITSU]
  - **説明**: AIは学習データに含まれる既存著作物に類似した出力を行う可能性がある。利用者が元の著作物を知らなくても、類似性と依拠性が認められれば著作権侵害となりうる。特に画像やデザインでは、類似性の確認が不可欠である。
  - **定義例**: 「AI生成物を対外利用する場合、既存著作物との類似性を可能な範囲で確認する。画像はインターネット画像検索で類似画像がないか確認し、文章は特徴的な表現が既存文献と一致しないか検証する」
- [ ] 商用利用時の追加確認事項が定められている [JDLA]
  - **説明**: AI生成物を商用利用する場合、非商用利用よりも高い注意義務が求められる。AIサービスの利用規約で商用利用が制限されている場合もあり、規約違反は契約上の責任だけでなく、ライセンス取消しにつながるリスクもある。
  - **定義例**: 「AI生成物を商用利用（顧客向け成果物、販売物、広告等）する場合は、①利用するAIサービスの規約で商用利用が許可されていることを確認、②著作権侵害リスクの追加チェックを実施、③商用利用であることをAI管理担当者に事前申告する」
- [ ] 画像・音声・動画生成時の特別な注意事項がある [JDLA] [EU-AIA]
  - **説明**: テキスト生成に比べ、画像・音声・動画の生成は著作権侵害やディープフェイクのリスクが格段に高い。実在する人物の肖像に似た画像の生成、既存キャラクターに酷似したデザインの生成は、肖像権・パブリシティ権・著作権の各侵害につながりうる。
  - **定義例**: 「画像・音声・動画を生成する場合は、テキスト生成時の注意事項に加え、①実在する人物の肖像に似た出力でないか確認、②既存のキャラクター・ブランドに類似していないか確認、③プロンプトで特定の作家名・作品名を指定しない。生成した画像等を社外で使用する場合は上長の承認を必須とする」
- [ ] 学習データの著作権に関する考慮がされている [METI] [FUJITSU]
  - **説明**: AIサービスの学習データに違法なコンテンツ（海賊版等）が含まれている可能性は否定できない。直接的な法的責任は主にAI開発者側にあるが、利用者として「学習データの出所が不明なサービスを選ばない」という視点を持つことが、リスク低減につながる。
  - **定義例**: 「AIサービス選定時に、提供者が学習データの適法性について説明しているかを確認する。学習データに関する情報が一切開示されていないサービスの利用は慎重に判断する」

### 4.3 出力の表示・開示

- [ ] AI生成コンテンツであることの表示要否の基準がある [EU-AIA] [METI]
  - **説明**: EU AI Actでは一定のAI生成コンテンツに表示義務が課される。日本では現時点で法的義務はないが、受け手がAI生成物と知らずに意思決定の根拠にした場合、透明性の欠如が信頼を損なう。どの場面で「AI利用」を明示すべきか、組織として基準を持つことが重要である。
  - **定義例**: 「以下の場合はAI生成物であることを明示する：①社外向けコンテンツ（広報・マーケティング資料等）、②顧客への提出物。社内利用のみの場合は表示を推奨とする。明示の方法は、文書末尾への注記（例：『本文書の草案作成にAIを利用しています』）とする」
- [ ] 社外向け資料でのAI利用に関するルールがある [JDLA]
  - **説明**: 社外向け資料にAI出力をそのまま使う場合、誤情報や不適切表現が組織の公式見解として受け取られるリスクがある。社内メモとは異なり、社外向け資料には組織としての品質保証が暗黙に求められる。
  - **定義例**: 「社外向け資料（提案書・報告書・プレスリリース・公開資料等）にAI出力を利用する場合、①内容の正確性確認と人間による加筆・編集を必須とし、②上長の承認を得てから提出・公開する」
- [ ] 顧客・取引先への説明責任に関する指針がある [NIST: GOVERN 5.1] [METI]
  - **説明**: 顧客が「専門家が作成した」と信頼して受け取ったレポートが実はAI出力のコピペだった場合、信頼関係は深刻に損なわれる。AIを利用したこと自体が問題ではなく、利用した事実を適切に伝えないことが問題になる。
  - **定義例**: 「顧客・取引先への成果物にAIを活用した場合、求めに応じてAI利用の事実を説明できるようにする。対面・電話での顧客対応にAIを活用する場合は、AI利用の旨を事前に伝える」
- [ ] ディープフェイク等の合成コンテンツへの対応がある [EU-AIA]
  - **説明**: AI技術により実在の人物の映像・音声を偽造する「ディープフェイク」は、詐欺・名誉毀損・情報操作に悪用される。自組織がディープフェイクを作成しないことはもちろん、外部から受け取った合成コンテンツへの対応も検討しておく必要がある。
  - **定義例**: 「実在する人物の顔・声を模倣した合成コンテンツの作成を禁止する。外部から受領した映像・音声コンテンツの真正性に疑義がある場合は、利用前に信頼できる情報源で確認する」

### 4.4 品質管理

- [ ] 重要度に応じた承認プロセスが定められている [NIST: MANAGE 1.1] [JDLA]
  - **説明**: すべてのAI出力に同じレベルの承認を求めるのは非効率だが、すべてを無審査で使うのはリスクが高い。出力の利用場面の重要度に応じて承認レベルを段階的に設けることで、効率と安全のバランスが取れる。
  - **定義例**: 「AI出力の利用を3段階の承認で管理する：①低リスク（社内参考資料）＝利用者自身の確認で可、②中リスク（社外向け資料の草案）＝上長の確認、③高リスク（公式文書・顧客提出物）＝上長承認＋専門家レビュー」
- [ ] 出力の記録・保存に関するルールがある [NIST: GOVERN 4.2] [IPA]
  - **説明**: AI出力を利用した業務で後日問題が発覚した場合、「どのプロンプトで何が出力され、どう使ったか」を遡れなければ原因究明も再発防止もできない。検証可能性を確保するためには、一定期間の記録保存が必要である。
  - **定義例**: 「重要な業務（社外提出物・意思決定への利用等）でAI出力を利用した場合、使用したプロンプトとAI出力の要旨、最終的な利用形態を記録し、少なくとも1年間保存する」
- [ ] 問題のある出力を発見した場合の報告プロセスがある [NIST: MANAGE 4.3] [JDLA]
  - **説明**: AIが差別的表現や明らかな誤情報を出力した場合、利用者がそれを報告できる仕組みがなければ、同じ問題が繰り返される。問題事例の蓄積は、ガイドライン改善の最も重要なインプットである。
  - **定義例**: 「AI出力に問題（明らかな誤情報、差別的・不適切な表現、個人情報の生成等）を発見した場合、AI管理担当者に報告する。報告はメール・チャット等で随時受け付け、管理担当者がインシデント記録に追加し、必要に応じてガイドラインの見直しに反映する」

---

## 5. 信頼性の確保 (Trustworthiness)

### 5.1 正確性・信頼性 (Valid & Reliable)

- [ ] ハルシネーションへの対処方法が具体的に示されている [NIST-GAI] [JDLA] [FUJITSU]
- [ ] 情報源の確認・引用の推奨がされている [JDLA] [FUJITSU]
- [ ] 生成AIの限界・不得意分野が説明されている [NIST: MAP 2.2] [FUJITSU]
- [ ] 出力の信頼度を判断するためのガイダンスがある [NIST: MEASURE 2.5]
- [ ] 継続的なテスト・検証の仕組みがある [NIST: MEASURE 2.4]

### 5.2 安全性 (Safe)

- [ ] 安全に関わる業務での利用制限が明記されている [NIST: MEASURE 2.6] [METI]
- [ ] 人間による最終判断が必要な場面が特定されている [NIST: MAP 3.5] [METI]
- [ ] 緊急停止・利用中止の基準と手順がある [NIST: MANAGE 2.4]
- [ ] 残存リスクの文書化がされている [NIST: MANAGE 1.4]

### 5.3 セキュリティ・レジリエンス (Secure & Resilient)

- [ ] 承認されたツール・サービスのリストがある [IPA] [JDLA]
- [ ] 未承認ツールの利用禁止が明記されている [IPA] [JDLA]
- [ ] アカウント管理（共有禁止、MFA等）のルールがある [IPA]
- [ ] プロンプトインジェクション等の攻撃への注意喚起がある [NIST-GAI] [IPA]
- [ ] API利用時のセキュリティ要件が定められている [IPA]
- [ ] データポイズニングへの対策が考慮されている [NIST-GAI]
- [ ] モデル抽出・メンバーシップ推論攻撃への対策がある [NIST-GAI]

### 5.4 透明性・説明責任 (Accountable & Transparent)

- [ ] AI利用の記録・ログ保存の要件がある [NIST: GOVERN 4.2] [METI] [EU-AIA]
- [ ] 意思決定過程の説明責任に関する指針がある [NIST: MEASURE 2.8] [METI]
- [ ] ステークホルダーへの情報開示方針がある [NIST: GOVERN 5.1] [METI]
- [ ] 学習データの出所（プロベナンス）の確認がある [NIST-GAI]

### 5.5 説明可能性 (Explainable & Interpretable)

- [ ] AI出力の根拠を説明する必要がある場面が特定されている [NIST: MEASURE 2.9] [METI]
- [ ] 説明が困難な場合の対応方針がある [NIST: MEASURE 2.9]
- [ ] 利用者の役割・知識レベルに応じた説明方法がある [NIST]

### 5.6 プライバシー保護 (Privacy-Enhanced)

- [ ] プライバシー影響評価の実施要否基準がある [NIST: MEASURE 2.10] [METI]
- [ ] データ最小化の原則が示されている [NIST] [METI]
- [ ] 個人の権利（アクセス、削除等）への対応方針がある [METI] [EU-AIA]
- [ ] プライバシー強化技術（PETs）の活用が検討されている [NIST]

### 5.7 公平性・バイアス管理 (Fair – Harmful Bias Managed)

- [ ] バイアスの種類と影響が説明されている [NIST] [METI] [FUJITSU]
- [ ] 人事・採用等の判断でのAI利用に関する注意事項がある [EU-AIA] [METI]
- [ ] バイアスを検出・軽減するための方策が示されている [NIST: MEASURE 2.11] [METI]
- [ ] 多様性・公平性の観点からのレビュー体制がある [NIST: GOVERN 3.1]
- [ ] システム的バイアス、統計的バイアス、認知バイアスの区別がある [NIST]

---

## 6. 具体的な禁止事項

### 6.1 絶対的禁止事項

- [ ] 違法行為への利用禁止が明記されている [JDLA] [METI]
- [ ] 他者を欺く目的での利用禁止が明記されている [JDLA] [EU-AIA]
- [ ] ディープフェイク等の悪用禁止が明記されている [EU-AIA] [JDLA]
- [ ] 差別・ハラスメントを助長する利用の禁止がある [NIST] [METI] [FUJITSU]
- [ ] マルウェア作成等のセキュリティ攻撃への利用禁止がある [JDLA] [IPA]
- [ ] サブリミナル技術等による操作的AI利用の禁止がある [EU-AIA]
- [ ] 社会的スコアリングへの利用禁止がある [EU-AIA]

### 6.2 業務上の禁止・制限事項

- [ ] 最終的な意思決定をAIのみに委ねることの禁止 [NIST: MAP 3.5] [METI]
- [ ] 出力の無検証での外部公開の禁止 [JDLA] [FUJITSU]
- [ ] 機密レベルの高い業務での利用制限 [JDLA] [IPA]
- [ ] 契約書・法的文書作成での制限事項 [JDLA]
- [ ] 顧客対応での制限事項 [JDLA]

---

## 7. インシデント対応

### 7.1 報告体制

- [ ] インシデントの定義・範囲が明確である [NIST: MANAGE 4.1] [IPA]
- [ ] 報告先・報告手順が具体的に示されている [NIST: MANAGE 4.3] [JDLA]
- [ ] 報告すべき事象の例示がある [IPA] [JDLA]
- [ ] 報告のタイムライン（即時、24時間以内等）が定められている [IPA]

### 7.2 対応プロセス

- [ ] 初動対応の手順が定められている [NIST: MANAGE 2.3] [IPA]
- [ ] 影響範囲の調査方法が示されている [NIST: MANAGE 4.1]
- [ ] 是正措置の実施プロセスがある [NIST: MANAGE 2.3]
- [ ] 再発防止策の策定プロセスがある [NIST: MANAGE 4.2]
- [ ] 必要に応じた外部報告（当局、顧客等）の基準がある [METI] [EU-AIA]
- [ ] システムの停止・無効化の基準と手順がある [NIST: MANAGE 2.4]

### 7.3 記録と改善

- [ ] インシデントの記録・保存要件がある [NIST: MANAGE 4.3] [IPA]
- [ ] 事後レビューの実施要件がある [NIST: MANAGE 4.2]
- [ ] ガイドラインへのフィードバックの仕組みがある [NIST: GOVERN 5.2]
- [ ] 外部ステークホルダーからのフィードバック収集がある [NIST: MEASURE 3.3]

---

## 8. 運用管理

### 8.1 ツール・サービス管理

- [ ] 利用可能なツール・サービスの一覧がある [IPA] [JDLA]
- [ ] 新規ツール導入時の承認プロセスがある [NIST: GOVERN 6.1] [IPA]
- [ ] 各ツールの利用条件・注意事項が整理されている [JDLA]
- [ ] ツールのバージョン管理・更新対応の方針がある [IPA]
- [ ] 事前学習済みモデルの監視体制がある [NIST: MANAGE 3.2]

### 8.2 アクセス管理

- [ ] 利用資格・権限の付与基準がある [IPA]
- [ ] アカウントの発行・管理プロセスがある [IPA]
- [ ] 退職・異動時の権限剥奪プロセスがある [IPA]
- [ ] 利用ログの取得・保存要件がある [IPA] [METI]

### 8.3 コスト管理

- [ ] 利用コストの管理・モニタリング方法がある [IPA]
- [ ] 予算超過時の対応が定められている [IPA]
- [ ] 部門別・プロジェクト別の費用按分ルールがある [IPA]

### 8.4 継続的改善

- [ ] ガイドラインの定期レビュースケジュールがある [NIST: GOVERN 1.5] [JDLA]
- [ ] 利用者からのフィードバック収集の仕組みがある [NIST: GOVERN 5.2] [JDLA]
- [ ] ベストプラクティスの共有・蓄積の仕組みがある [NIST: GOVERN 4.3]
- [ ] 技術動向・法規制動向のウォッチ体制がある [METI]
- [ ] 継続的な測定・評価の仕組みがある [NIST: MEASURE 4.3]

---

## 9. 特定用途・業界固有の考慮事項

### 9.1 コード生成・開発業務

- [ ] 生成コードのセキュリティレビュー要件がある [NIST-GAI] [IPA]
- [ ] ライセンス・著作権の確認プロセスがある [JDLA]
- [ ] 本番環境への適用に関する制限がある [IPA]
- [ ] 脆弱性が含まれる可能性への注意喚起がある [NIST-GAI] [IPA]

### 9.2 文書作成・コンテンツ生成

- [ ] 社外向け文書での利用ルールがある [JDLA]
- [ ] マーケティング・広告での利用ルールがある [JDLA]
- [ ] 翻訳業務での品質確認要件がある [JDLA]

### 9.3 データ分析・レポート作成

- [ ] 分析結果の検証要件がある [NIST: MEASURE 2.5] [JDLA]
- [ ] 統計的誤りへの注意喚起がある [NIST-GAI]
- [ ] 意思決定への利用に関する制限がある [METI]

### 9.4 顧客対応・カスタマーサービス

- [ ] チャットボット等での利用ルールがある [METI]
- [ ] 人間によるエスカレーション基準がある [NIST: MAP 3.5]
- [ ] 顧客への開示要件がある [EU-AIA] [METI]

### 9.5 業界固有の規制対応（該当する場合）

- [ ] 金融規制（金融庁ガイドライン等）への対応 [METI]
- [ ] 医療規制（薬機法等）への対応 [METI] [EU-AIA]
- [ ] 個人情報保護規制（GDPR等）への対応 [METI] [EU-AIA]
- [ ] 輸出管理規制への対応 [METI]
- [ ] その他業界固有の規制への対応 [METI]

---

## 10. 文書としての完成度

### 10.1 構成・可読性

- [ ] 目次・構成が分かりやすい [JDLA]
- [ ] 用語の定義が明確である [NIST] [JDLA]
- [ ] 具体例・FAQが含まれている [JDLA]
- [ ] 関連文書へのリンク・参照が整備されている [JDLA]

### 10.2 実効性

- [ ] 現場で実際に使える具体性がある [JDLA]
- [ ] 判断に迷う場面での指針が示されている [JDLA]
- [ ] 問い合わせ先・相談窓口が明記されている [JDLA]
- [ ] 例外対応のプロセスが定められている [NIST: GOVERN 1.3]

### 10.3 管理情報

- [ ] 文書のバージョン・改訂履歴がある [JDLA]
- [ ] 制定日・施行日が明記されている [JDLA]
- [ ] 次回見直し予定が記載されている [NIST: GOVERN 1.5] [JDLA]
- [ ] 承認者・責任者が明記されている [JDLA]

---

## チェックリスト活用のヒント

1. すべての項目を網羅する必要はありません。自社の業務内容、リスク許容度、リソースに応じて優先順位をつけてください。

2. 「×（未対応）」の項目は、意図的に対象外としたのか、検討漏れなのかを区別してください。

3. 技術の進歩や法規制の変化に伴い、定期的（少なくとも年1回）にこのチェックリストを使って見直しを行うことを推奨します。

4. NIST AI RMFの4つの機能（GOVERN, MAP, MEASURE, MANAGE）を意識した構成になっています。特にGOVERN（ガバナンス）は全体を通じて重要です。

5. [EU-AIA]が付いている項目は、EU市場でビジネスを行う場合に特に重要です。

---

## 参考ガイドライン一覧

### 米国

| 名称                                                                  | 発行元           | 発行日      | URL                                                   |
| --------------------------------------------------------------------- | ---------------- | ----------- | ----------------------------------------------------- |
| AI Risk Management Framework (AI RMF 1.0)                             | NIST             | 2023年1月   | <https://www.nist.gov/itl/ai-risk-management-framework> |
| AI RMF Playbook                                                       | NIST             | 2023年1月～ | <https://airc.nist.gov/AI_RMF_Knowledge_Base/Playbook>  |
| Generative AI Profile (NIST-AI-600-1)                                 | NIST             | 2024年7月   | <https://doi.org/10.6028/NIST.AI.600-1>                 |
| Secure Software Development Practices for Generative AI (SP 800-218A) | NIST             | 2024年7月   | <https://csrc.nist.gov/pubs/sp/800/218/a/final>         |
| Blueprint for an AI Bill of Rights                                    | White House OSTP | 2022年10月  | <https://www.whitehouse.gov/ostp/ai-bill-of-rights/>    |

### EU・国際機関

| 名称                                            | 発行元       | 発行日        | URL                                              |
| ----------------------------------------------- | ------------ | ------------- | ------------------------------------------------ |
| EU AI Act (AI規制法)                            | EU           | 2024年8月発効 | <https://eur-lex.europa.eu/eli/reg/2024/1689/oj>   |
| AI Act Compliance Checker                       | EU AI Office | 2024年～      | <https://artificialintelligenceact.eu/>            |
| OECD AI Principles                              | OECD         | 2019年5月     | <https://oecd.ai/en/ai-principles>                 |
| Hiroshima Process International Code of Conduct | G7           | 2023年12月    | <https://www.mofa.go.jp/ecm/ec/page5e_000076.html> |
| ISO/IEC 42001:2023 (AI Management System)       | ISO          | 2023年12月    | <https://www.iso.org/standard/81230.html>          |

### 日本（政府・公的機関）

| 名称                                 | 発行元                         | 発行日    | URL                                                                                               |
| ------------------------------------ | ------------------------------ | --------- | ------------------------------------------------------------------------------------------------- |
| AI事業者ガイドライン（第1.1版）      | 経済産業省・総務省             | 2025年3月 | <https://www.meti.go.jp/shingikai/mono_info_service/ai_shakai_jisso/>                               |
| 人間中心のAI社会原則                 | 統合イノベーション戦略推進会議 | 2019年3月 | <https://www8.cao.go.jp/cstp/ai/aiprinciples/index.html>                                            |
| テキスト生成AI導入・運用ガイドライン | IPA                            | 2024年7月 | <https://www.ipa.go.jp/jinzai/ics/core_human_resource/final_project/2024/textgen-ai-guideline.html> |
| AIと著作権に関する考え方について     | 文化庁                         | 2024年3月 | <https://www.bunka.go.jp/seisaku/chosakuken/aitotyosakuken.html>                                    |

### 日本（民間団体）

| 名称                                | 発行元                           | 発行日     | URL                                                                               |
| ----------------------------------- | -------------------------------- | ---------- | --------------------------------------------------------------------------------- |
| 生成AIの利用ガイドライン（第1.1版） | 日本ディープラーニング協会(JDLA) | 2023年10月 | <https://www.jdla.org/document/>                                                    |
| 生成AI利活用ガイドライン（第1.2版） | 富士通                           | 2024年7月  | <https://www.fujitsu.com/jp/about/research/technology/ai/generative-ai-guidelines/> |

### 業界別参考資料

| 名称                                   | 発行元     | 対象業界 | URL                                               |
| -------------------------------------- | ---------- | -------- | ------------------------------------------------- |
| 金融分野におけるAI活用に関する報告書   | 金融庁     | 金融     | <https://www.fsa.go.jp/news/r5/singi/20240401.html> |
| 医療AIの開発・運用に関するガイドライン | 厚生労働省 | 医療     | <https://www.mhlw.go.jp/stf/newpage_08269.html>     |

---

## 更新履歴

| バージョン | 日付      | 変更内容 |
| ---------- | --------- | -------- |
| 1.0        | 2026年1月 | 初版作成 |

---

_本チェックリスト作成日: 2026年1月_
_主要参照フレームワーク: NIST AI RMF 1.0, NIST-AI-600-1, AI事業者ガイドライン第1.1版, JDLA生成AI利用ガイドライン第1.1版_
