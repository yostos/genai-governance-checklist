# 用語集

本チェックリストで使用する
生成AI関連の主要な用語を解説します。

| 用語 | 説明 |
| ---- | ---- |
| **生成AI（Generative AI）** | テキスト・画像・音声などの新しいコンテンツを生成するAI技術の総称。ChatGPT・Claude・Gemini等が代表例 |
| **大規模言語モデル（LLM）** | 大量のテキストデータで学習し、自然言語を理解・生成するAIモデル。生成AIの中核技術 |
| **基盤モデル（Foundation Model）** | 汎用的に学習された大規模AIモデル。さまざまな用途に応用される土台となるモデル |
| **プロンプト** | 生成AIに対する指示文・入力文。プロンプトの書き方により出力の品質が大きく変わる |
| **ハルシネーション（Hallucination）** | 生成AIが事実に基づかない情報をもっともらしく出力する現象。不具合ではなくAIの構造的な特性である |
| **ファクトチェック** | AI出力の事実関係を一次情報源で検証すること。ハルシネーション対策の基本 |
| **ヒューマン・イン・ザ・ループ（HITL）** | AIの判断・出力に対して人間が最終確認・意思決定を行う運用原則 |
| **AIリテラシー（AI Literacy）** | AIの能力・限界・リスクを理解し適切に利用する能力。EU AI Act第4条で義務化されている概念 |
| **プロンプトインジェクション** | 悪意ある指示をプロンプトに埋め込み、AIのガードレールを回避させる攻撃手法 |
| **データポイズニング** | AIの学習データに悪意あるデータを混入させ、出力を操作する攻撃手法 |
| **モデル抽出攻撃** | AIモデルの挙動を分析し、モデル自体を複製・再現しようとする攻撃手法 |
| **メンバーシップ推論攻撃** | AIモデルの出力から、特定のデータが学習データに含まれていたかを推測する攻撃手法 |
| **ガードレール** | AIが有害・不適切な出力をしないように設けられた安全制約の仕組み |
| **ディープフェイク（Deepfake）** | AI技術を用いて実在する人物の顔・声を模倣した合成コンテンツ。詐欺や名誉毀損に悪用されるリスクがある |
| **バイアス（AIにおける）** | 学習データの偏りに起因し、AIが特定の属性（性別・人種等）に対して不公平な出力をする傾向 |
| **シャドーAI（Shadow AI）** | 組織が承認していないAIツール・サービスを従業員が無断で業務利用すること。セキュリティ管理の死角となる |
| **オプトアウト** | AIサービスに入力したデータをモデルの学習に利用しないよう設定すること |
| **プロベナンス（Provenance）** | AIモデルの学習データの出所・来歴。学習データの品質や著作権上の問題を把握するための情報 |
| **プライバシー強化技術（PETs）** | 差分プライバシーや合成データ生成など、プライバシーを技術的に保護する手法の総称 |
| **合成データ（Synthetic Data）** | 実データの統計的特徴を模した人工的に生成されたデータ。プライバシー保護に有用 |
