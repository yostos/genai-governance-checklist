# 2. リスクの特定と評価 (MAP)

## 2.1 利用目的・文脈の明確化

- [ ] 生成AIを利用する業務・ユースケースが特定されている [NIST: MAP 1.1] [JDLA]
  - **説明**: 「何にでも使っていい」は「何に使うか考えていない」と同義である。利用する業務を特定しないまま導入すると、効果測定もリスク評価もできず、問題が起きたとき「想定外の使い方だった」と言い訳するしかなくなる。
  - **定義例**: 「生成AIの利用を認める業務として、①文書の草稿作成、②調査・情報収集の補助、③翻訳・要約の支援、④アイデア出し・ブレインストーミングを指定する。それ以外の業務での利用はAI管理担当者の事前承認を要する」
- [ ] 各ユースケースにおける期待される効果（便益）が文書化されている [NIST: MAP 3.1] [METI]
  - **説明**: 「便利だから」だけでは導入の根拠にならない。期待する効果を具体的に書き出しておくことで、導入後に「実際に役立っているか」を評価でき、効果が薄い用途の見直しや、有効な用途の拡大につなげられる。
  - **定義例**: 「各ユースケースについて、期待する効果（例：議事録作成の所要時間を半減、定型文書の初稿作成を自動化）を記録し、半年後に実績と比較して効果を検証する」
- [ ] 想定ユーザーとその利用方法が定義されている [NIST: MAP 1.1] [METI]
  - **説明**: 同じAIツールでも、使う人の知識レベルや業務内容によってリスクの大きさが変わる。経理担当者が財務データを入力するリスクと、広報担当者がプレスリリースの草案を作るリスクは本質的に異なる。想定ユーザーを明確にすることで、リスクに応じた対策が可能になる。
  - **定義例**: 「主な利用者は事務職員（文書作成補助）および広報担当（コンテンツ草案作成）とする。各利用者は、自身の業務範囲内でのみAIを利用し、他部門のデータを扱う場合は当該部門の承認を得る」
- [ ] AIを使用すべきでない業務・場面が明記されている [NIST: MAP 1.1] [JDLA]
  - **説明**: 「使ってよい場面」を列挙するだけでは、グレーゾーンの判断に迷う。「使ってはいけない場面」を明示することで、禁止ラインが明確になり、現場が安心して判断できるようになる。ネガティブリストの欠如は、重大なリスクを見落とす原因になる。
  - **定義例**: 「以下の業務・場面ではAI利用を禁止する：①個人の評価・処遇に関する判断、②法的効力を持つ文書の最終作成、③本人確認・認証に関わる業務、④外部への正式な回答・声明の作成」
- [ ] ビジネス価値・目的が明確に定義されている [NIST: MAP 1.4]
  - **説明**: AI導入が「流行に乗る」ための施策になっていないかを確認する視点である。組織にとっての具体的な価値——コスト削減、品質向上、業務時間の創出——が言語化されていなければ、投資対効果の判断も撤退判断もできない。
  - **定義例**: 「生成AI導入のビジネス価値を、①定型業務の作業時間削減による本来業務への集中、②文書品質の底上げによる手戻り削減と定義し、導入後に定量的な評価を行う」
- [ ] 組織のミッション・目標との整合性が確認されている [NIST: MAP 1.3]
  - **説明**: AI利用が組織の理念や目標と矛盾していないかを確認する。たとえば「人に寄り添うサービス」を掲げる組織が、顧客対応を全面的にAIに置き換えるのは本末転倒である。ツールの導入が組織の方向性を歪めないよう、上位の目標と照合する習慣が必要である。
  - **定義例**: 「生成AIの利用方針が当団体の事業計画・行動指針と整合していることを、年次の方針見直し時に確認する」

## 2.2 リスクの分類と評価

- [ ] 情報漏えいリスクの評価がされている [JDLA] [IPA] [FUJITSU]
  - **説明**: 生成AIに入力した情報は、サービス提供元のサーバーに送信される。「学習に利用しない」設定だけでは不十分で、秘密保持義務のないサービスに機密情報を入力すれば、実質的に情報が外部に渡ったのと同じである。どの情報がどの経路で漏えいしうるかを具体的に洗い出すことが出発点となる。
  - **定義例**: 「生成AI利用における情報漏えいリスクを、情報種別（機密・社内限・公開）×漏えい経路（入力データの学習利用・履歴閲覧・サーバー保存）のマトリクスで評価し、対策を定める」
- [ ] 著作権・知的財産権侵害リスクの評価がされている [JDLA] [FUJITSU] [METI]
  - **説明**: 生成AIの出力は既存の著作物と類似する可能性がある。利用者が元の著作物を知らなくても、AIが学習データに基づいて類似物を生成した場合、著作権侵害の「依拠性あり」と推認されうる（文化庁見解）。「AIが出したから大丈夫」は通用しない。
  - **定義例**: 「生成AI出力物の著作権侵害リスクを評価する。特に画像・デザイン・文章の生成では、既存著作物との類似性を確認するプロセスを設け、疑わしい場合は利用を控える」
- [ ] ハルシネーション（誤情報生成）リスクの評価がされている [NIST-GAI] [JDLA] [FUJITSU]
  - **説明**: 生成AIは「もっともらしい嘘」を自信満々に出力する。正誤判断を行わず確率予測で文章を生成する仕組み上、これは不具合ではなく構造的な特性である。どの業務で誤情報のリスクが高いか、誤情報が流通した場合の影響はどの程度かを予め評価しておくことが重要である。
  - **定義例**: 「ハルシネーションリスクを業務別に評価する。事実確認が容易な業務（要約・翻訳）は低リスク、専門知識が必要な業務（法務・技術調査）は高リスクと分類し、高リスク業務では必ず専門家によるファクトチェックを行う」
- [ ] バイアス・公平性に関するリスクの評価がされている [NIST: MAP 5.1] [METI] [FUJITSU]
  - **説明**: 学習データの偏りにより、AIは特定の属性（性別・人種・年齢等）に対して不公平な出力を行うことがある。「CEOの画像」を生成すると白人男性ばかり、「看護師」を生成すると女性ばかりになるのは代表的な例である。人事・評価・顧客対応にAIを使う場合、バイアスは直接的な差別につながりうる。
  - **定義例**: 「AI出力が特定の属性に不利益をもたらすバイアスを含んでいないか、利用前に確認する。人に関する判断補助にAIを用いる場合は、バイアスリスクを『高』と評価し、複数の視点からのレビューを義務づける」
- [ ] プライバシーリスクの評価がされている [NIST: MAP 5.1] [METI]
  - **説明**: 生成AIは、入力されたデータだけでなく、学習データに含まれる個人情報を出力に反映することもある。意図せず他者のプライバシーを侵害する可能性があり、入力側・出力側の両面でリスクを評価する必要がある。
  - **定義例**: 「プライバシーリスクを入力面（個人情報を含むデータの入力有無）と出力面（個人を特定しうる情報の生成有無）の2軸で評価し、リスクが高い業務では匿名化または利用制限を適用する」
- [ ] セキュリティリスク（プロンプトインジェクション等）の評価がされている [NIST-GAI] [IPA]
  - **説明**: 生成AIには、悪意あるプロンプトでガードレールを回避させる「プロンプトインジェクション」や、学習データを汚染する「データポイズニング」など、従来のITとは異なるセキュリティ脅威がある。これらの攻撃手法を理解し、自組織の利用形態で影響を受けうるかを評価しておく必要がある。
  - **定義例**: 「生成AIに特有のセキュリティリスク（プロンプトインジェクション、情報抽出攻撃等）を洗い出し、自組織の利用形態（社内利用のみ／顧客向けチャットボット等）に応じた影響度を評価する」
- [ ] 法的リスク（契約違反、規制違反等）の評価がされている [NIST: GOVERN 1.1] [METI]
  - **説明**: AI利用が契約上の守秘義務違反、著作権法違反、個人情報保護法違反、あるいは業界固有の規制違反に該当する可能性がないかを事前に評価する。「法令違反の認識がなかった」は免責事由にならない。
  - **定義例**: 「AI利用に関連する法的リスクを、個人情報保護法・著作権法・不正競争防止法・各業界規制の観点で評価し、リスクが高い領域を特定して対策を講じる」
- [ ] レピュテーションリスクの評価がされている [NIST: MAP 5.1] [FUJITSU]
  - **説明**: AIが不正確な情報や不適切なコンテンツを生成し、それが組織名で外部に出た場合、信用の失墜は法的リスク以上に深刻な打撃を与えうる。検証せずにAIの回答をそのまま社会に出すことは、顧客や社会からの「信頼」を損なう行為である。
  - **定義例**: 「AI出力を外部に公開する業務（広報・顧客対応・公式文書等）について、誤情報・不適切表現によるレピュテーションリスクを評価し、公開前に必ず人間によるレビューを実施する」
- [ ] 業務依存リスク（過度な依存、スキル低下）の評価がされている [NIST-GAI] [FUJITSU]
  - **説明**: AIに頼りすぎると、人間自身の判断力や業務スキルが徐々に低下する。AIサービスが停止した場合に業務が止まる「依存リスク」と、長期的にスキルが衰退する「能力喪失リスク」の両面から評価が必要である。
  - **定義例**: 「AI利用が長期化した場合の業務依存度を評価する。AIなしでも最低限の業務遂行が可能な体制を維持し、定期的にAIを使わない業務訓練や手動プロセスの確認を行う」
- [ ] 環境負荷・サステナビリティの考慮がされている [NIST: MEASURE 2.12] [METI]
  - **説明**: 大規模言語モデルの学習・推論には膨大なエネルギーと水資源が消費される。組織の環境方針やSDGs目標との整合性を確認し、必要以上に大きなモデルを不必要に利用していないかを検討する視点も重要である。
  - **定義例**: 「AI利用の環境負荷を認識し、業務に必要十分なモデル・サービスを選択する。組織の環境方針がある場合は、AI利用方針との整合性を確認する」

## 2.3 サードパーティリスク

- [ ] 利用するAIサービス・ツールの選定基準が定められている [NIST: GOVERN 6.1] [IPA]
  - **説明**: 「無料だから」「話題だから」で選んだツールが、実は入力データを学習に利用していたり、セキュリティ対策が不十分だったりするケースは多い。選定基準がなければ、部門ごとにバラバラなツールが乱立し、管理コストとリスクが膨らむ。
  - **定義例**: 「AIツールの選定にあたっては、①データの取扱方針（学習利用の有無）、②セキュリティ対策（通信暗号化・データ保存場所）、③利用規約の内容、④提供元の信頼性を確認し、AI管理担当者が承認した上で利用を開始する」
- [ ] ベンダーの利用規約・プライバシーポリシーの確認プロセスがある [NIST: MAP 4.1] [JDLA]
  - **説明**: 利用規約を読まずに「同意する」をクリックするのは個人の自由だが、組織の業務データを扱うサービスでは許されない。規約の中に「入力データを学習に利用する」「生成物の権利はサービス提供者に帰属する」といった条項が含まれている可能性がある。
  - **定義例**: 「AIツール導入前に、AI管理担当者が利用規約・プライバシーポリシーを確認し、データの学習利用有無、生成物の権利帰属、サービス提供者のデータアクセス範囲を把握する。重大な懸念がある場合は顧問弁護士に相談する」
- [ ] データの取り扱い（学習利用の有無等）の確認がされている [JDLA] [IPA]
  - **説明**: 入力データが学習に使われる場合、自社の情報が他のユーザーへの応答に反映される可能性がある。「学習に利用しない」オプションがあっても、デフォルト設定で有効になっているとは限らない。設定画面まで確認してはじめて「確認済み」と言える。
  - **定義例**: 「利用する各AIサービスについて、入力データの学習利用有無、データ保存期間、サーバー所在地を確認・記録する。学習利用のオプトアウト設定がある場合は確実にオフに設定し、その設定状況を記録する」
- [ ] サービス停止・変更時の対応策が検討されている [NIST: GOVERN 6.2]
  - **説明**: 特定のAIサービスに業務を依存した状態でサービスが突然停止・仕様変更されると、業務が止まる。クラウドサービスには永続の保証がない。代替手段や移行計画を事前に考えておくことが、事業継続の観点から不可欠である。
  - **定義例**: 「主要AIツールの停止・大幅変更に備え、①代替ツールの候補を予め把握し、②重要業務についてはAIなしでも遂行できる手順を維持する。サービス停止時はAI管理担当者が48時間以内に代替策を提示する」
- [ ] サプライチェーン全体でのリスク管理が考慮されている [NIST: MAP 4.2] [EU-AIA]
  - **説明**: 自組織が利用するAIサービスの裏側には、基盤モデル提供者・クラウドインフラ事業者・データ提供者が連なっている。自組織の直接の契約先だけでなく、その先の構造的リスク——基盤モデルの脆弱性、学習データの品質問題——も影響しうることを認識する必要がある。
  - **定義例**: 「利用するAIサービスについて、基盤モデルの提供元・データ処理の所在地・再委託先の有無を可能な範囲で把握し、サプライチェーン上のリスクをAI管理台帳に記録する」
