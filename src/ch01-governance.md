# 1. ガバナンス体制 (GOVERN)

## 1.1 組織体制・責任

- [ ] 生成AI利用に関する最終責任者（経営層）が明確に定められている [NIST: GOVERN 2.3] [METI]
  - **説明**: AI関連の問題発生時に最終判断を下す責任者を即答できる状態をつくる
    。責任者が不明確だと、インシデント発生時に判断が宙に浮き、対応の遅れが法的・社
    会的リスクを組織全体に波及させる。また、名目上の責任者を置くだけでも不十分——実
    務を担当者へ任せきりにし、責任者自身がリスクや運用実態を把握していない体制は
    「責任者不在」と同義である。
  - **定義例**: 「団体理事長をAI利用の最終責任者とし、利用方針の承認・リスク受容の判断・重大インシデント時の意思決定を自ら行う。定期的に運用状況の報告を受け、方針の妥当性を確認する」
- [ ] AI利用に関する担当部門・担当者が指定されている [NIST: GOVERN 2.1] [JDLA]
  - **説明**: 最終責任者の下で、日常のAI利用を実務面で管理する部門・担当者を明確にする。指定がないと、利用上の疑問やトラブルの相談先が不在となり、現場が自己判断で使い続ける"野良AI"状態を招く。
  <!-- textlint-disable ja-technical-writing/ja-no-successive-word -->
  - **定義例**: 「総務部をAI利用の管理部門とし、担当者○○がツール選定・利用ルールの周知・問い合わせ対応・インシデント一次対応を担う」
  <!-- textlint-enable ja-technical-writing/ja-no-successive-word -->
- [ ] 部門横断的な推進体制（法務、情報システム、事業部門等）が構築されている [NIST: GOVERN 3.1] [IPA]
  - **説明**: AI利用は情報セキュリティ・法務・業務の各領域にまたがるため、単一部門だけでは死角が生まれる。「IT部門が技術面だけ見て法的リスクを見落とす」「事業部門が便利さだけ追求しセキュリティを無視する」といった事態が典型的な失敗パターンである。小規模組織であっても、外部の専門家（顧問弁護士等）を含めた連携体制を意識的に構築すべきである。
  - **定義例**: 「AI利用に関する方針検討・リスク判断は、総務部（管理）・業務担当部門・外部顧問（法務・IT）が合同で行い、各視点からの確認を経て決定する」
- [ ] 役割と責任の範囲が文書化されている [NIST: GOVERN 2.1] [METI]
  - **説明**: 「誰が何をやるか」を文書化しておかないと、問題が起きたときに「それは私の担当ではない」と責任の押しつけ合いが始まる。口頭の合意だけでは人事異動や担当変更で引き継がれず、属人的な運用に逆戻りする。
  - **定義例**: 「AI利用に関する役割分担表を作成し、最終責任者・管理担当者・各部
    門の利用者それぞれの権限と義務を明記する。担当者変更時は必ず更新し、全関係者に
    共有する」
- [ ] インシデント発生時のエスカレーションルートが定められている [NIST: GOVERN 1.5] [IPA]
  - **説明**: 情報漏えいや誤出力の発生時に「誰に・どの順番で・何を報告するか」が決まっていなければ、初動の遅れにより被害が拡大する。現場で問題に気づいた人が報告先を探しているうちに、対応のゴールデンタイムを逃すのは典型的な失敗である。
  - **定義例**: 「AIに関する問題を発見した場合、①まずAI管理担当者に口頭またはチャットで即時報告、②管理担当者が30分以内に最終責任者へ報告、③最終責任者が対応方針を判断し指示する。連絡先リストを全従業員に配布する」
- [ ] 多様な視点を持つチーム構成が考慮されている [NIST: GOVERN 3.1]
  - **説明**: AIのリスクは技術的な問題だけでなく、倫理・文化・利用者の多様性に関わる問題も含む。同質的なメンバーだけで意思決定すると、特定の立場からは明白なリスクが見過ごされやすい。性別・年齢・職種・専門分野など、多様な視点を意識的にチーム構成に取り入れることが重要である。
  - **定義例**: 「AI利用ルールの策定・見直しにあたっては、管理部門だけでなく、異なる業務経験・年齢層・職種の職員を含むレビュー体制とし、偏った視点にならないよう配慮する」

## 1.2 ポリシー・規程

- [ ] 生成AI利用の目的・方針が明記されている [NIST: GOVERN 1.2] [JDLA]
  - **説明**: 「なぜ生成AIを使うのか」「どういう方針で使うのか」が明文化されていないガイドラインは、単なるルール集にすぎず現場の納得感を得られない。目的なき規制は形骸化し、目的なき自由は暴走を招く。
  - **定義例**: 「本ガイドラインは、業務効率の向上と創造性の支援を目的として生成AIを活用する際の基本方針を定める。AIはあくまで人間の判断を補助するツールとして位置づけ、最終的な意思決定は人間が行う」
- [ ] ガイドラインの適用範囲（対象者、対象業務、対象ツール）が明確である [JDLA] [IPA]
  - **説明**: 「誰が」「どの業務で」「どのツールについて」このガイドラインに従うのかが不明確だと、「自分は対象外」と解釈する余地が生まれ、ルールが空文化する。正規職員だけが対象なのか、委託先や実習生も含むのか——範囲の曖昧さはリスクの温床になる。
  - **定義例**: 「本ガイドラインは、当団体のすべての役職員（常勤・非常勤・委託スタッフを含む）が、業務上利用するすべての生成AIサービス（ChatGPT、Copilot等、団体が承認したツール）に適用する」
- [ ] 既存の情報セキュリティポリシー、個人情報保護方針との整合性が取れている [NIST: GOVERN 1.4] [METI]
  - **説明**: 生成AIガイドラインが既存の情報セキュリティポリシーや個人情報保護方針と矛盾していると、現場は「どちらに従えばいいのか」と混乱する。既存ポリシーで「外部クラウドへの機密情報送信禁止」と定めているのに、AIガイドラインでその点に触れていなければ、現場が独自解釈で運用してしまう。
  - **定義例**: 「本ガイドラインは既存の情報セキュリティポリシー及び個人情報保護方針との整合性を確認済みである。AI固有の事項については本ガイドラインが優先し、記載のない事項は既存ポリシーに従う」
- [ ] 関連する法令・規制要件（個人情報保護法、著作権法等）への言及がある [NIST: GOVERN 1.1] [JDLA]
  - **説明**: 生成AI利用には個人情報保護法、著作権法、不正競争防止法など複数の法令が関わる。ガイドラインで法令に触れていないと、「法的リスクは考慮済み」と現場が誤解し、知らないうちに法令違反を犯すおそれがある。すべてを網羅する必要はないが、最低限の関連法令を明示し、判断に迷う場合の相談先を示すことが重要である。
  - **定義例**: 「生成AIの利用にあたっては、個人情報保護法・著作権法・不正競争防止法を遵守する。法的判断に迷う場合は、AI管理担当者を通じて顧問弁護士に相談する」
- [ ] ガイドラインの定期的な見直し・更新の仕組みが定められている [NIST: GOVERN 1.5] [METI]
  - **説明**: 生成AI技術と関連法規制は急速に変化するため、策定時点のガイドラインは半年もすれば陳腐化しうる。「一度つくって終わり」のガイドラインは変化に対応できず形骸化する。更新のタイミング・手順・責任者を予め決めておくことが、ガイドラインを"生きた文書"にする条件である。
  - **定義例**: 「本ガイドラインは少なくとも年1回（毎年○月）に見直しを行う。AIに関する重大な法改正やインシデント発生時には臨時の見直しを実施する。見直しはAI管理担当者が起案し、最終責任者が承認する」
- [ ] リスク許容度が組織として定義されている [NIST: MAP 1.5] [METI]
  - **説明**: 「どの程度のリスクなら許容するか」を組織として決めていないと、担当者ごとに判断基準がバラつき、同じ案件でも人によって可否が分かれる。リスク許容度が明確であれば現場判断に一貫性が生まれ、個人の裁量に過度に依存する状態を防げる。
  - **定義例**: 「公開情報の要約・翻訳支援など、情報漏えいリスクが低く出力誤りの影響も限定的な業務はAI利用を許可する。個人情報・機密情報を扱う業務、対外的な意思決定に関わる業務ではAI利用を制限または禁止する。判断が困難な場合はAI管理担当者の承認を得る」

## 1.3 教育・研修

- [ ] 従業員向けの生成AI利用研修が計画されている [NIST: GOVERN 2.2] [JDLA]
  - **説明**: ガイドラインを策定しただけでは機能しない。読まれない規程は存在しないのと同じである。研修を通じて「なぜこのルールがあるのか」を理解させなければ、現場はルールを面倒な制約としか捉えず、形だけの遵守か無視が常態化する。
  - **定義例**: 「生成AI利用開始前に全職員を対象とした初回研修（90分）を実施する。内容はガイドラインの要点、禁止事項、入力してはいけない情報の具体例、インシデント報告手順とする。受講記録を管理し、未受講者にはAI利用権限を付与しない」
- [ ] ITリテラシーレベルに応じた教育内容が用意されている [JDLA] [IPA]
  - **説明**: 全員に同じ研修を実施しても、ITに詳しい人には退屈で、不慣れな人には理解が追いつかない。結果として両者とも研修の効果を実感できず、「やった感」だけが残る。対象者のレベルに合わせた教育設計が、実効性ある研修の前提条件である。
  - **定義例**: 「研修は2段階で実施する。基礎編（全職員対象）ではAIの概要・リスク・禁止事項を扱い、実践編（日常的にAIを使う職員対象）ではプロンプト設計・出力検証の方法・業務活用事例を扱う」
- [ ] 新規利用者向けのオンボーディングプロセスがある [NIST: GOVERN 2.2]
  - **説明**: 新しく入った職員が、既存メンバーと同じ前提知識を持っているとは限らない。入職後に「AI使っていいですよ」とだけ言われ、ガイドラインの存在すら知らないまま使い始めるケースは十分ありうる。新規利用者が必ず通る導入プロセスを設けることで、知識のばらつきを防ぐ。
  - **定義例**: 「新規入職者には、入職時オリエンテーションの一環としてAI利用ガイドラインの説明と基礎研修の受講を義務づける。研修完了後にAIツールのアカウントを発行する」
- [ ] 継続的な啓発活動（事例共有、注意喚起等）の仕組みがある [NIST: GOVERN 4.3] [JDLA]
  - **説明**: 一度研修を受けても、日常業務に追われるうちルールの記憶は薄れる。他社のAI関連トラブル事例や、自組織での良い活用例を定期的に共有することで、ガイドラインの存在意義を繰り返し意識させる。啓発が途絶えると、ガイドラインは「入社時に読んだきりの文書」に退化する。
  - **定義例**: 「四半期に1回、AI利用に関するミニ勉強会（30分）を実施し、最新のリスク事例・活用好事例・ガイドライン改訂内容を共有する。また、重大な外部事例が発生した場合は、速やかに全職員にメール等で注意喚起する」

## 1.4 監視・監査

- [ ] 利用状況のモニタリング方法が定められている [NIST: GOVERN 1.5] [IPA]
  - **説明**: 「ガイドラインを守っているか」を確認する手段がなければ、ルールは形骸化する。誰がどのツールをどの頻度で使い、どんな用途に使っているかを把握できなければ、問題の早期発見や改善ができない。
  - **定義例**: 「AI管理担当者は月1回、各AIツールの利用ログ（利用者・利用頻度・主な用途）を確認し、ガイドライン違反の有無や想定外の利用がないかを点検する」
- [ ] 定期的な監査・レビューの実施計画がある [NIST: GOVERN 1.5] [METI]
  - **説明**: モニタリングが日常の健康管理だとすれば、監査は定期健診にあたる。日々の点検では見落とす構造的な問題——ルール自体の陳腐化や、組織全体の運用傾向——を発見するには、定期的に一歩引いた視点でレビューする場が必要である。
  - **定義例**: 「年1回（○月）にAI利用に関する内部監査を実施する。ガイドライン遵守状況、リスク対応状況、利用効果を評価し、結果を最終責任者に報告する」
- [ ] 違反時の対応プロセスが明記されている [JDLA] [IPA]
  - **説明**: 違反が発覚したとき、対応が場当たり的だと「あの人は注意されたのに、この人はお咎めなし」と不公平感が生まれ、ルールの信頼性が崩壊する。軽微な違反と重大な違反を区別し、段階的な対応を予め定めておくことが公正な運用の基盤になる。
  - **定義例**: 「違反を確認した場合、①軽微な違反（過失による禁止情報の入力等）は口頭注意と再研修、②重大な違反（意図的な機密情報の入力、悪用目的の利用等）はAI利用権限の停止と懲戒規定に基づき対応する。すべての違反対応を記録する」
- [ ] AIシステムのインベントリ管理の仕組みがある [NIST: GOVERN 1.6]
  - **説明**: 組織内で「誰が・何のAIツールを・どの業務で使っているか」を一覧で把握できていないと、未承認ツールの利用や、サービス終了への対応が後手に回る。管理対象を可視化することがガバナンスの出発点である。
  - **定義例**: 「利用中のAIツール・サービスの一覧表を作成し、ツール名・提供元・利用部門・利用目的・契約状況・データ取扱条件を記載する。新規導入・廃止時に随時更新する」
- [ ] AIシステムの安全な廃止・移行プロセスがある [NIST: GOVERN 1.7]
  - **説明**: AIツールの利用を終了する際に、保存されたプロンプト履歴や生成データをそのまま放置すれば、情報漏えいのリスクが残り続ける。「使い始め」だけでなく「使い終わり」にもルールが必要である。
  - **定義例**: 「AIツールの利用を終了する場合、①ツール上に保存された業務データ・履歴を削除またはエクスポート、②アカウントの無効化、③代替手段への移行計画を策定し、AI管理担当者が完了を確認する」
